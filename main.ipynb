{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reciped-ML: A Natural Language Processing (NLP) model for text embedding of recipes used in our recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install scikit-learn\n",
    "%pip install annoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import joblib\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict \n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleanup and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_list(string: str):\n",
    "\tstring = string[1:-1]\n",
    "\tstring = string.split(\",\")\n",
    "\tstring = [x.strip() for x in string]\n",
    "\treturn string;\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "recipe_df = pd.read_csv('recdata.csv', converters={'ingredients': convert_str_to_list, 'directions': convert_str_to_list, 'NER': convert_str_to_list})\n",
    "recipe_df = recipe_df.sort_values(by='id', ascending=True)\n",
    "recipe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Type casting values for appropriate use\n",
    "# recipe_df['title'] = recipe_df['title'].astype(str)\n",
    "# recipe_df['ingredients'] = recipe_df['ingredients'].astype(ast.literal_eval)\n",
    "# recipe_df['directions'] = recipe_df['directions'].astype(ast.literal_eval)\n",
    "# recipe_df['NER'] = recipe_df['NER'].astype(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words found in NER column\n",
    "measurement_units = [\"spoon\", \"cup\", \"ounce\", \"gram\", \"handful\", \"pinch\", \"tasty\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Cleanup function for ingredient name\n",
    "def clean_ingredient_name(ingredient: str) -> str:\n",
    "    if 'http' in ingredient:\n",
    "        return None\n",
    "    if len(ingredient) <= 1:\n",
    "        return None\n",
    "    ingredient = re.sub(r'[_$\"\\',#…\\\\/(){}\\[\\]!?0-9]', '', ingredient) # Remove special characters, symbols, and digits\n",
    "    ingredient = ingredient.strip() # Trim\n",
    "    ingredient = ingredient.lower() # Lowercase\n",
    "    ingredient = lemmatizer.lemmatize(ingredient) # Base word\n",
    "    ingredient = re.sub(r'\\b(?:' + '|'.join(map(re.escape, measurement_units)) + r')\\b', '', ingredient) # Remove ingredient names\n",
    "    if 'and' in ingredient:\n",
    "        ingredient = ingredient.split('and')\n",
    "        ingredient = [ing.strip() for ing in ingredient]\n",
    "        ingredient = [re.sub(r'[_$\"\\',#…\\\\/(){}\\[\\]!?0-9]', '', ing) for ing in ingredient]\n",
    "        ingredient = [ing.lower() for ing in ingredient]\n",
    "        ingredient = [lemmatizer.lemmatize(ing) for ing in ingredient]\n",
    "        ingredient = [re.sub(r'\\b(?:' + '|'.join(map(re.escape, measurement_units)) + r')\\b', '', ing) for ing in ingredient]\n",
    "    return ingredient\n",
    "\n",
    "# Flatten the list of ingredients in the NER column\n",
    "def flatten_list(lst: list) -> list:\n",
    "    flattened = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(item)\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    flattened = [ing for ing in flattened if ing is not None]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying cleanup function\n",
    "print(recipe_df['NER'][0])\n",
    "recipe_df['NER'] = recipe_df['NER'].apply(lambda x: flatten_list([clean_ingredient_name(item) for item in x]))\n",
    "recipe_df = recipe_df.dropna(subset=['NER'])\n",
    "recipe_df = recipe_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrating unique ingredient dataset\n",
    "recipe_df_exploded = recipe_df.explode(column='NER')\n",
    "recipe_df_exploded['NER'] = recipe_df_exploded['NER'].str.lower().str.strip()\n",
    "unique_ingredients = recipe_df_exploded['NER'].unique()\n",
    "ingredients_df = pd.DataFrame({'ingredient': unique_ingredients})\n",
    "ingredients_df['id'] = range(1, len(ingredients_df) + 1)\n",
    "ingredients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_df = ingredients_df[['id', 'ingredient']]\n",
    "ingredients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writeing cleaned datasets to csv for storage\n",
    "recipe_df.to_pickle('recipes_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was noticed that there were certain ingredients that were extremely common amongst recipes, and when used in embeddings, they somewhat overpowered the more important ingredients. It was decided that these ingredients do not play much of a role in providing much distinguising power for recipes, and would not always be required by the user to be inputted. Thus, the top 15 most common ingredients were completely renmoved from the NER column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common(ingredients: list) -> list:\n",
    "    return [ingredient for ingredient in ingredients if ingredient not in most_common_ingredients ]\n",
    "\n",
    "# Tallying of ingredient frequency amongst recipes\n",
    "vocabulary = nltk.FreqDist()\n",
    "for ingredients in recipe_df['NER']:\n",
    "    vocabulary.update(ingredients)\n",
    "\n",
    "most_common_ingredients = [word for word, freq in vocabulary.most_common(15)]\n",
    "most_common_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the top 15 most common\n",
    "recipe_df['NER'] = recipe_df[\"NER\"].apply(remove_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes use of a popular Neural Network (NN) called word2vec that takes in a corpus of text and maps each word in the corpus to a vector of fixed length. The model captures semantic and syntatic relationships between words with the idea that words with similar meanings tend to occur in similar contexts. For this model, the NER column represents a list of base ingredients that have been extract using other forms of ML. These lists are considered to be the documents of the corpus, as it was decided that the amount of ingredients and preparation style did not contribute significantly to the overall theme of the recipe. The word2vec model replaces each word in the corpus with a vector that is 100 elements in length. In saying that, the documents become two dimensional, which proves to be computationally expensive to compare when using techniques such as cosine similarity. For that reason, a weighted average method was adopted in the form of TFIDF embeddings, where the words are given a weight directly proportional to their term frequency in the document and inversely proportional to their frequency across documents. This embedding was used to give a weighted average of vectors in a single document, resulting in a single one dimensional vector that is 100 elements in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates document embeddings from word embeddings using a weighted average\n",
    "class TfidfEmbedding(object):\n",
    "    def __init__(self, model_w2v: Word2Vec):\n",
    "\n",
    "        self.model_w2v = model_w2v\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = model_w2v.wv.vector_size\n",
    "\n",
    "    # creates the idf dictionary for all words in the corpus\n",
    "    def fit(self, recipes: list):\n",
    "        text_docs = []\n",
    "        for doc in recipes:\n",
    "            text_docs.append(\" \".join(doc)) # become space seperated strings\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(text_docs)  \n",
    "        joblib.dump(tfidf, 'tfidf.pkl')\n",
    "        max_idf = max(tfidf.idf_)   # if a word was never seen it is given idf of the max of known idf value\n",
    "        self.word_idf_weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    # converts a list of recipes to a list of document vectors\n",
    "    def transform(self, recipes: list): \n",
    "        doc_word_vector = self.doc_average_list(recipes)\n",
    "        return doc_word_vector\n",
    "\n",
    "    # retruns the document embedding as a weighted average\n",
    "    def doc_average(self, recipe: list):\n",
    "        mean = []\n",
    "        for word in recipe:\n",
    "            if word in self.model_w2v.wv.index_to_key:\n",
    "                mean.append(\n",
    "                    self.model_w2v.wv.get_vector(word) * self.word_idf_weight[word]\n",
    "                ) \n",
    "\n",
    "        if not mean:  \n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "    # returns the full list of all document embeddings\n",
    "    def doc_average_list(self, recipes):\n",
    "        return np.vstack([self.doc_average(recipe) for recipe in recipes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingredient lists are sorted alphabetically to ensure a standard order. Since word2vec considers surrounding words for context, identical ingredients in different orders are considerd to have different context. Additionally, when generating recommendations, a simple cosine similarity check on all possible recipes is computationally expensive to repeat a number of times. This is why the ANNOY (Approximate Nearest Neighbour Oh Yeah) python library is used. This library is designed to efficiently find the approximate nearest neighbours of a query point in high dimensional space. The annoy index is a datastructure that stores randomized binary trees for efficient retrieval of nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts the ingredient list alphabetically\n",
    "def sort_corpus(recipes: pd.DataFrame) -> list:\n",
    "    sorted_corpus = []\n",
    "    for doc in recipes[\"NER\"]:\n",
    "        doc.sort()\n",
    "        sorted_corpus.append(doc)\n",
    "    print(sorted_corpus[0])\n",
    "    return sorted_corpus\n",
    "\n",
    "# Window length is taken to be the average length of ingredient lists\n",
    "def get_window_length(corpus: list) -> int:\n",
    "    recipe_lengths =[len(doc) for doc in corpus]\n",
    "    avg_length = float(sum(recipe_lengths)/len(recipe_lengths))\n",
    "    return round(avg_length)\n",
    "\n",
    "# Creates and saves the word2vec model\n",
    "def create_and_save_w2v_model(recipes: pd.DataFrame) -> Word2Vec:\n",
    "    corpus = sort_corpus(recipes)\n",
    "    model_w2v = Word2Vec(corpus, sg=0, workers=8, window=get_window_length(corpus), min_count=1, vector_size=100)\n",
    "    model_w2v.init_sims(replace=True)\n",
    "    model_w2v.save('models/model_w2v.bin')\n",
    "    return model_w2v \n",
    "\n",
    "# Generates the full corupus embeddings for all recipes\n",
    "def get_corpus_embeddings(recipes: pd.DataFrame, tfidf_vectorizer: TfidfEmbedding) -> list:\n",
    "        corpus = sort_corpus(recipes)\n",
    "        fitted_tfidf = tfidf_vectorizer.fit(corpus)\n",
    "        recipe_embeddings = tfidf_vectorizer.transform(corpus)\n",
    "        recipe_embeddings = [doc.reshape(1, -1) for doc in recipe_embeddings]\n",
    "        assert len(recipe_embeddings) == len(corpus)\n",
    "        return recipe_embeddings, fitted_tfidf\n",
    "\n",
    "# Returns a list of recipe_ids relating to the top N recommendations \n",
    "def get_recommendations(input: list, fitted_tfidf: TfidfEmbedding, annoy_index: AnnoyIndex, corpus_embeddings: list, N=5, chef=False) -> list:\n",
    "    # Input is a list of ingredients if chef is true , else it is a recipe_id\n",
    "    if chef:\n",
    "        input.sort()\n",
    "        input_embedding = fitted_tfidf.transform([input])[0].reshape(1, -1)[0]\n",
    "    else:\n",
    "        input_embedding = corpus_embeddings[input]\n",
    "        \n",
    "    return annoy_index.get_nns_by_vector(input_embedding, 5)\n",
    "\n",
    "# Creates the corpus embeddings as well as builds and saves an annoy index\n",
    "def create_and_save_embeddings(model_w2v : Word2Vec, recipes : pd.DataFrame) -> (list, TfidfEmbedding, AnnoyIndex):\n",
    "    tfidf_vectorizer = TfidfEmbedding(model_w2v)\n",
    "    corpus_embeddings, fitted_tfidf = get_corpus_embeddings(recipes, tfidf_vectorizer)\n",
    "\n",
    "    annoy_index = AnnoyIndex(100, 'angular') # Initialize empty index of dimension 100\n",
    "    for i, embedding in enumerate(corpus_embeddings): # Add embeddings to the index\n",
    "        annoy_index.add_item(i, embedding[0])\n",
    "        \n",
    "    annoy_index.build(n_trees=500) # Build the index to allow for effective search\n",
    "    annoy_index.save('annoy_index.ann')\n",
    "    return corpus_embeddings, fitted_tfidf, annoy_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = create_and_save_w2v_model(recipe_df)\n",
    "corpus_embeddings, fitted_tfidf, annoy_index = create_and_save_embeddings(model_w2v, recipe_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = [list(embedding[0]) for embedding in corpus_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('embed.csv', \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe = recipe_df.loc[100, ['title', 'NER']]\n",
    "print(f\"Test recipe is : {test_recipe}\")\n",
    "input_id = 100\n",
    "recommended_ids = get_recommendations(input=input_id, fitted_tfidf=fitted_tfidf, annoy_index=annoy_index, corpus_embeddings=corpus_embeddings, chef=False)\n",
    "\n",
    "for id in recommended_ids:\n",
    "    print(f\"Recommended Recipe is: {recipe_df.loc[id, ['title', 'NER']]}\")\n",
    "\n",
    "input = [\"strawberry\", \"mango\"]\n",
    "print(f\"Test ingredient list is {input}\")\n",
    "recommended_ids = get_recommendations(input=input, fitted_tfidf=fitted_tfidf, annoy_index=annoy_index, corpus_embeddings=corpus_embeddings, chef=True)\n",
    "\n",
    "for id in recommended_ids:\n",
    "    print(f\"Recommended Recipe is: {recipe_df.loc[id, ['title', 'NER']]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
